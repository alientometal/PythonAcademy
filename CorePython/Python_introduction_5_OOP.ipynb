{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20f5292e",
   "metadata": {},
   "source": [
    "# Python OOP\n",
    "\n",
    "**Goal:** Use Object-Oriented Programming (OOP) to model reusable components such as **connectors**, **pipeline steps**, **validators**, and **AI workflow building blocks**.\n",
    "\n",
    "**Prerequisites:** You already know basic Python syntax (functions, loops, dictionaries, lists).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f78726",
   "metadata": {},
   "source": [
    "## Learning objectives\n",
    "\n",
    "By the end of this lesson, you should be able to:\n",
    "\n",
    "- Explain **class vs object (instance)** and what `self` means.\n",
    "- Create classes with **attributes** and **methods** using `__init__`.\n",
    "- Distinguish **instance attributes** vs **class attributes**.\n",
    "- Use `@classmethod` and `@staticmethod` appropriately.\n",
    "- Apply **inheritance** + `super()` to reuse behavior.\n",
    "- Use **polymorphism** to build pluggable pipeline steps.\n",
    "- Use **encapsulation** with `@property` to enforce invariants (validation).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59aa7618",
   "metadata": {},
   "source": [
    "## Why OOP matters in Data Engineering / AI\n",
    "\n",
    "In real projects you rarely write one-off scripts. You build **reusable components**:\n",
    "\n",
    "- **Extractors**: read from files, APIs, databases.\n",
    "- **Transformers**: clean, normalize, enrich, feature-engineer.\n",
    "- **Validators**: schema checks, null checks, quality gates.\n",
    "- **Loaders**: write to a warehouse, data lake, vector DB.\n",
    "- **AI steps**: chunking, embedding, reranking, prompt formatting.\n",
    "\n",
    "OOP gives you a clean way to package:\n",
    "\n",
    "- **State** (configuration, credentials, batch sizes, paths)\n",
    "- **Behavior** (read/transform/write/run)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e2cc82",
   "metadata": {},
   "source": [
    "## Quick mental model: class vs instance\n",
    "\n",
    "- A **class** is a **blueprint** (definition).\n",
    "- An **instance/object** is a **concrete thing** built from that blueprint.\n",
    "\n",
    "Think of a pipeline step:\n",
    "\n",
    "- Class: `FilterColumns` (the idea of filtering columns)\n",
    "- Instance: `FilterColumns(columns=[\"id\", \"amount\"])` (a specific configuration)\n",
    "\n",
    "**Key takeaway:** Most bugs happen when we confuse *shared state* (class level) with *per-object state* (instance level).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d97af9df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'id': 1, 'amount': 10, 'source': 'web'}, {'id': 2, 'amount': 15, 'source': 'web'}]\n"
     ]
    }
   ],
   "source": [
    "# A tiny pipeline-step interface we'll reuse throughout the notebook\n",
    "\n",
    "class PipelineStep:\n",
    "    '''A minimal interface for a pipeline step.\n",
    "\n",
    "    Each step receives `data` and returns transformed data.\n",
    "    '''\n",
    "    def run(self, data):\n",
    "        raise NotImplementedError(\"Subclasses must implement run(data)\")\n",
    "\n",
    "\n",
    "class AddConstantColumn(PipelineStep):\n",
    "    def __init__(self, column_name, value):\n",
    "        self.column_name = column_name\n",
    "        self.value = value\n",
    "\n",
    "    def run(self, rows):\n",
    "        # rows is a list[dict], a common \"row-like\" representation in DE demos\n",
    "        out = []\n",
    "        for r in rows:\n",
    "            r2 = dict(r)\n",
    "            r2[self.column_name] = self.value\n",
    "            out.append(r2)\n",
    "        return out\n",
    "\n",
    "\n",
    "rows = [{\"id\": 1, \"amount\": 10}, {\"id\": 2, \"amount\": 15}]\n",
    "step = AddConstantColumn(column_name=\"source\", value=\"web\")\n",
    "print(step.run(rows))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73e929af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MissingRun(PipelineStep):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a27f3ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "MR = MissingRun()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6c1f3d2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Subclasses must implement run(data)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotImplementedError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m s = \u001b[43mMR\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mPipelineStep.run\u001b[39m\u001b[34m(self, data)\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m, data):\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mSubclasses must implement run(data)\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNotImplementedError\u001b[39m: Subclasses must implement run(data)"
     ]
    }
   ],
   "source": [
    "s = MR.run({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "89dd9800",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Subclasses must implement run(data)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotImplementedError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[43mMR\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m, \u001b[33m\"\u001b[39m\u001b[33mSubclasses must implement run(data)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mPipelineStep.run\u001b[39m\u001b[34m(self, data)\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m, data):\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mSubclasses must implement run(data)\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNotImplementedError\u001b[39m: Subclasses must implement run(data)"
     ]
    }
   ],
   "source": [
    "assert MR.run({}), \"Subclasses must implement run(data)\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6245f4d2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1) Defining a class\n",
    "\n",
    "Basic syntax:\n",
    "\n",
    "```python\n",
    "class MyClass:\n",
    "    ...\n",
    "```\n",
    "\n",
    "- Use **CapWords** for class names: `CSVReader`, `VectorIndexer`.\n",
    "- Keep behavior close to the data/config it uses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6331c53b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '__main__.Empty'>\n"
     ]
    }
   ],
   "source": [
    "class Empty:\n",
    "    pass\n",
    "\n",
    "\n",
    "e = Empty()\n",
    "print(type(e))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5f0bdf",
   "metadata": {},
   "source": [
    "## 2) `__init__`, `self`, attributes, and methods\n",
    "\n",
    "- `__init__` runs when you create the object.\n",
    "- `self` is the *current instance*.\n",
    "- Attributes store the instance state (configuration).\n",
    "- Methods implement behavior.\n",
    "\n",
    "We'll model a **CSV extraction config**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f83c3c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSVExtractor(path=data/sales.csv, delimiter=;, encoding=utf-8)\n",
      "data/sales.csv\n"
     ]
    }
   ],
   "source": [
    "class CSVExtractor:\n",
    "    def __init__(self, path, delimiter=\",\", encoding=\"utf-8\"):\n",
    "        self.path = path\n",
    "        self.delimiter = delimiter\n",
    "        self.encoding = encoding\n",
    "\n",
    "    def describe(self):\n",
    "        return f\"CSVExtractor(path={self.path}, delimiter={self.delimiter}, encoding={self.encoding})\"\n",
    "\n",
    "\n",
    "extractor = CSVExtractor(\"data/sales.csv\", delimiter=\";\")\n",
    "print(extractor.describe())\n",
    "print(extractor.path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a63ab11",
   "metadata": {},
   "source": [
    "### Common beginner mistakes\n",
    "\n",
    "- Forgetting parentheses: `obj.method` vs `obj.method()`\n",
    "- Forgetting `self` in method signature\n",
    "- Shadowing attributes with local variables (e.g., using `path = ...` instead of `self.path = ...`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2604edb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method CSVExtractor.describe of <__main__.CSVExtractor object at 0x7777e0200510>>\n",
      "CSVExtractor(path=data/sales.csv, delimiter=;, encoding=utf-8)\n"
     ]
    }
   ],
   "source": [
    "# obj.method vs obj.method()\n",
    "\n",
    "print(extractor.describe)    # this is a bound method object\n",
    "print(extractor.describe())  # this executes the method\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1692ab35",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3) Instance attributes vs class attributes\n",
    "\n",
    "- **Instance attribute**: stored on each object (different per instance).\n",
    "- **Class attribute**: stored on the class (shared across instances).\n",
    "\n",
    "In DE/AI, class attributes are often used for **defaults** (timeouts, retries).\n",
    "Be careful: if it’s mutable, you can accidentally share state across objects.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7b7b82d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "class APIClient:\n",
    "    default_timeout_seconds = 10  # class attribute (shared default)\n",
    "\n",
    "    def __init__(self, base_url, timeout_seconds=None):\n",
    "        self.base_url = base_url\n",
    "        self.timeout_seconds = timeout_seconds if timeout_seconds is not None else APIClient.default_timeout_seconds\n",
    "\n",
    "\n",
    "c1 = APIClient(\"https://api.example.com\", timeout_seconds=5)\n",
    "c2 = APIClient(\"https://api.example.com\")\n",
    "\n",
    "print(c1.timeout_seconds)  # 5 (instance override)\n",
    "print(c2.timeout_seconds)  # 10 (class default)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0662c0f",
   "metadata": {},
   "source": [
    "### Pitfall: mutable class attributes (shared state)\n",
    "\n",
    "If you use a mutable object (like a list/dict) as a class attribute, **all instances share it**.\n",
    "This is a very common bug.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "484e9186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.tags: ['critical']\n",
      "b.tags: ['critical']\n"
     ]
    }
   ],
   "source": [
    "class BadConfig:\n",
    "    tags = []  # shared across all instances (danger!)\n",
    "\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "\n",
    "\n",
    "a = BadConfig(\"job_a\")\n",
    "b = BadConfig(\"job_b\")\n",
    "\n",
    "a.tags.append(\"critical\")\n",
    "print(\"a.tags:\", a.tags)\n",
    "print(\"b.tags:\", b.tags)  # surprise: it changed too\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ec81d5",
   "metadata": {},
   "source": [
    "**Fix:** use an instance attribute instead.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59b18124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.tags: ['critical']\n",
      "b.tags: []\n"
     ]
    }
   ],
   "source": [
    "class GoodConfig:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.tags = []  # unique per instance\n",
    "\n",
    "\n",
    "a = GoodConfig(\"job_a\")\n",
    "b = GoodConfig(\"job_b\")\n",
    "\n",
    "a.tags.append(\"critical\")\n",
    "print(\"a.tags:\", a.tags)\n",
    "print(\"b.tags:\", b.tags)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfc2bdf",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4) `@classmethod`: alternative constructors and class-wide config\n",
    "\n",
    "A **class method**:\n",
    "\n",
    "- Receives the class as `cls` (not the instance).\n",
    "- Is commonly used for **alternative constructors** like:\n",
    "  - `from_config(dict)`\n",
    "  - `from_env()`\n",
    "  - `from_uri(uri)`\n",
    "\n",
    "This is extremely common in DE/AI codebases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2696b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "db.company.net etl analytics\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "class WarehouseConnector:\n",
    "    default_schema = \"public\"\n",
    "\n",
    "    def __init__(self, host, user, password, schema=None):\n",
    "        self.host = host\n",
    "        self.user = user\n",
    "        self.password = password\n",
    "        self.schema = schema if schema is not None else WarehouseConnector.default_schema\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, cfg: dict):\n",
    "        return cls(\n",
    "            host=cfg[\"host\"],\n",
    "            user=cfg[\"user\"],\n",
    "            password=cfg[\"password\"],\n",
    "            schema=cfg.get(\"schema\")\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def from_env(cls):\n",
    "        # Example env vars: WAREHOUSE_HOST, WAREHOUSE_USER, WAREHOUSE_PASSWORD\n",
    "        return cls(\n",
    "            host=os.environ.get(\"WAREHOUSE_HOST\", \"localhost\"),\n",
    "            user=os.environ.get(\"WAREHOUSE_USER\", \"admin\"),\n",
    "            password=os.environ.get(\"WAREHOUSE_PASSWORD\", \"admin\"),\n",
    "        )\n",
    "\n",
    "\n",
    "cfg = {\"host\": \"db.company.net\", \"user\": \"etl\", \"password\": \"secret\", \"schema\": \"analytics\"}\n",
    "conn = WarehouseConnector.from_config(cfg)\n",
    "print(conn.host, conn.user, conn.schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "19cac94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WAREHOUSE_PASSWORD\"] = \"not_admin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "207e8be4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'not_admin'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ.get(\"WAREHOUSE_PASSWORD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8e278a5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "localhost admin public\n"
     ]
    }
   ],
   "source": [
    "conn = WarehouseConnector.from_env()\n",
    "print(conn.host, conn.user, conn.schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1acd2b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5) `@staticmethod`: helper functions in a class namespace\n",
    "\n",
    "A **static method**:\n",
    "\n",
    "- Receives **no** `self` and **no** `cls`.\n",
    "- Is a function that conceptually belongs to the class domain.\n",
    "- Typical uses: parsing, validation, small conversions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03589508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/raw/events.parquet\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "class PathUtils:\n",
    "    @staticmethod\n",
    "    def normalize_slashes(path: str) -> str:\n",
    "        return path.replace(\"\\\\\", \"/\")\n",
    "\n",
    "    @staticmethod\n",
    "    def is_parquet(path: str) -> bool:\n",
    "        return PathUtils.normalize_slashes(path).lower().endswith(\".parquet\")\n",
    "\n",
    "\n",
    "print(PathUtils.normalize_slashes(r\"data\\raw\\events.parquet\"))\n",
    "print(PathUtils.is_parquet(\"data/raw/events.parquet\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3aebf17f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data\\\\raw\\\\events.parquet'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r\"data\\raw\\events.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9cfe4e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\\raw\\events.parquet\n"
     ]
    }
   ],
   "source": [
    "print(r\"data\\raw\\events.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b872d7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6) Inheritance + `super()`: reuse and extend behavior\n",
    "\n",
    "Inheritance models **\"is-a\"** relationships:\n",
    "\n",
    "- `ExtractStep` is-a `PipelineStep`\n",
    "- `TransformStep` is-a `PipelineStep`\n",
    "\n",
    "We'll build a small hierarchy:\n",
    "- `PipelineStep` (base)\n",
    "- `NamedStep` (adds step naming + logging)\n",
    "- concrete steps (extract/transform)\n",
    "\n",
    "`super()` is how you call parent logic safely.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "419bb630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[filter_cols] Keeping columns: ['amount', 'id']\n",
      "[{'id': 1, 'amount': 10}, {'id': 2, 'amount': 15}]\n"
     ]
    }
   ],
   "source": [
    "class NamedStep(PipelineStep):\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "\n",
    "    def _log(self, message):\n",
    "        print(f\"[{self.name}] {message}\")\n",
    "\n",
    "\n",
    "class FilterColumns(NamedStep):\n",
    "    def __init__(self, name, keep_columns):\n",
    "        super().__init__(name=name)  # call parent init\n",
    "        self.keep_columns = set(keep_columns)\n",
    "\n",
    "    def run(self, rows):\n",
    "        self._log(f\"Keeping columns: {sorted(self.keep_columns)}\")\n",
    "        out = []\n",
    "        for r in rows:\n",
    "            out.append({k: v for k, v in r.items() if k in self.keep_columns})\n",
    "        return out\n",
    "\n",
    "\n",
    "rows = [{\"id\": 1, \"amount\": 10, \"country\": \"MX\"}, {\"id\": 2, \"amount\": 15, \"country\": \"US\"}]\n",
    "step = FilterColumns(name=\"filter_cols\", keep_columns=[\"id\", \"amount\"])\n",
    "print(step.run(rows))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f0acdd",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7) Polymorphism: same interface, different implementations\n",
    "\n",
    "Polymorphism is the foundation of **pluggable pipelines**.\n",
    "\n",
    "If every step implements:\n",
    "\n",
    "```python\n",
    "step.run(data) -> data\n",
    "```\n",
    "\n",
    "Then we can build a generic pipeline runner that does not care about the concrete class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b4ad50d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[cols] Keeping columns: ['amount', 'id']\n",
      "[mul] Multiplying amount by 2\n",
      "[{'id': 1, 'amount': 20, 'source': 'batch_2026_01_27'}, {'id': 2, 'amount': 30, 'source': 'batch_2026_01_27'}]\n"
     ]
    }
   ],
   "source": [
    "def run_pipeline(steps, data):\n",
    "    for step in steps:\n",
    "        data = step.run(data)\n",
    "    return data\n",
    "\n",
    "\n",
    "class MultiplyAmount(NamedStep):\n",
    "    def __init__(self, name, factor):\n",
    "        super().__init__(name=name)\n",
    "        self.factor = factor\n",
    "\n",
    "    def run(self, rows):\n",
    "        self._log(f\"Multiplying amount by {self.factor}\")\n",
    "        out = []\n",
    "        for r in rows:\n",
    "            r2 = dict(r)\n",
    "            r2[\"amount\"] = r2[\"amount\"] * self.factor\n",
    "            out.append(r2)\n",
    "        return out\n",
    "\n",
    "\n",
    "pipeline = [\n",
    "    FilterColumns(name=\"cols\", keep_columns=[\"id\", \"amount\"]),\n",
    "    MultiplyAmount(name=\"mul\", factor=2),\n",
    "    AddConstantColumn(column_name=\"source\", value=\"batch_2026_01_27\"),\n",
    "]\n",
    "\n",
    "rows = [{\"id\": 1, \"amount\": 10, \"country\": \"MX\"}, {\"id\": 2, \"amount\": 15, \"country\": \"US\"}]\n",
    "print(run_pipeline(pipeline, rows))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854dc1a0",
   "metadata": {},
   "source": [
    "### Duck typing (Pythonic)\n",
    "\n",
    "In Python we often rely on duck typing:\n",
    "\n",
    "> “If it has a `run(data)` method, we can run it.”\n",
    "\n",
    "That’s why you can mix different step classes in the same pipeline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2288bd03",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8) Encapsulation with `@property`: enforce invariants\n",
    "\n",
    "Encapsulation means: keep your internal state consistent and validated.\n",
    "\n",
    "Python does not enforce strict privacy, but it provides conventions:\n",
    "\n",
    "- `_x` means “internal/protected by convention”\n",
    "- `__x` triggers **name mangling** (harder to access accidentally)\n",
    "\n",
    "`@property` lets you expose a clean API while validating assignments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "242a7617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 False\n",
      "2000 True\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "batch_size must be a positive integer",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     23\u001b[39m cfg.batch_size = \u001b[32m2000\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;28mprint\u001b[39m(cfg.batch_size, cfg.is_large_batch)\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m \u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbatch_size\u001b[49m = -\u001b[32m1\u001b[39m  \u001b[38;5;66;03m# uncomment to see the validation error\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mBatchConfig.batch_size\u001b[39m\u001b[34m(self, value)\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;129m@batch_size\u001b[39m.setter\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbatch_size\u001b[39m(\u001b[38;5;28mself\u001b[39m, value):\n\u001b[32m     11\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m value <= \u001b[32m0\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mbatch_size must be a positive integer\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     13\u001b[39m     \u001b[38;5;28mself\u001b[39m._batch_size = value\n",
      "\u001b[31mValueError\u001b[39m: batch_size must be a positive integer"
     ]
    }
   ],
   "source": [
    "class BatchConfig:\n",
    "    def __init__(self, batch_size):\n",
    "        self.batch_size = batch_size  # triggers setter\n",
    "\n",
    "    @property\n",
    "    def batch_size(self):\n",
    "        return self._batch_size\n",
    "\n",
    "    @batch_size.setter\n",
    "    def batch_size(self, value):\n",
    "        if not isinstance(value, int) or value <= 0:\n",
    "            raise ValueError(\"batch_size must be a positive integer\")\n",
    "        self._batch_size = value\n",
    "\n",
    "    @property\n",
    "    def is_large_batch(self):\n",
    "        return self._batch_size >= 1000\n",
    "\n",
    "\n",
    "cfg = BatchConfig(batch_size=500)\n",
    "print(cfg.batch_size, cfg.is_large_batch)\n",
    "\n",
    "cfg.batch_size = 2000\n",
    "print(cfg.batch_size, cfg.is_large_batch)\n",
    "\n",
    "cfg.batch_size = -1  # uncomment to see the validation error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "41c191c6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "batch_size must be a positive integer",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbatch_size\u001b[49m = -\u001b[32m5\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mBatchConfig.batch_size\u001b[39m\u001b[34m(self, value)\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;129m@batch_size\u001b[39m.setter\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbatch_size\u001b[39m(\u001b[38;5;28mself\u001b[39m, value):\n\u001b[32m     11\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m value <= \u001b[32m0\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mbatch_size must be a positive integer\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     13\u001b[39m     \u001b[38;5;28mself\u001b[39m._batch_size = value\n",
      "\u001b[31mValueError\u001b[39m: batch_size must be a positive integer"
     ]
    }
   ],
   "source": [
    "cfg.batch_size = -5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a4687bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg._batch_size = -5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00c85cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67cbcd7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9) Embedding preparation (toy example)\n",
    "\n",
    "We’ll keep it simple: pretend we are preparing records for an embedding model.\n",
    "\n",
    "- Input rows: `{\"id\": ..., \"text\": ...}`\n",
    "- Output rows: add `text_len` and `chunk_id` fields\n",
    "\n",
    "The point is not the embedding itself; it’s the **OOP structure** that supports AI workflows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "79225af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[prep_embed] Chunking text with max_chars=40\n",
      "[{'id': 1, 'chunk_id': 0, 'text': 'hello hello hello hello hello hello hell', 'text_len': 40}, {'id': 1, 'chunk_id': 1, 'text': 'o hello hello hello hello hello hello he', 'text_len': 40}, {'id': 1, 'chunk_id': 2, 'text': 'llo hello hello hello hello hello hello ', 'text_len': 40}]\n",
      "total chunks: 8\n"
     ]
    }
   ],
   "source": [
    "class PrepareForEmbedding(NamedStep):\n",
    "    def __init__(self, name, max_chars=100):\n",
    "        super().__init__(name=name)\n",
    "        self.max_chars = max_chars\n",
    "\n",
    "    @staticmethod\n",
    "    def _chunk_text(text: str, max_chars: int):\n",
    "        # naive chunking: split into fixed-size pieces\n",
    "        return [text[i:i + max_chars] for i in range(0, len(text), max_chars)]\n",
    "\n",
    "    def run(self, rows):\n",
    "        self._log(f\"Chunking text with max_chars={self.max_chars}\")\n",
    "        out = []\n",
    "        for r in rows:\n",
    "            chunks = self._chunk_text(r[\"text\"], self.max_chars)\n",
    "            for idx, chunk in enumerate(chunks):\n",
    "                out.append({\n",
    "                    \"id\": r[\"id\"],\n",
    "                    \"chunk_id\": idx,\n",
    "                    \"text\": chunk,\n",
    "                    \"text_len\": len(chunk),\n",
    "                })\n",
    "        return out\n",
    "\n",
    "\n",
    "rows = [{\"id\": 1, \"text\": \"hello \" * 50}]\n",
    "prep = PrepareForEmbedding(name=\"prep_embed\", max_chars=40)\n",
    "out = prep.run(rows)\n",
    "print(out[:3])\n",
    "print(\"total chunks:\", len(out))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a246adfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello '"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"hello \" * 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb366dad",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10) (Optional) `@dataclass` for configuration objects\n",
    "\n",
    "In DE/AI projects, you often have many config objects.\n",
    "`@dataclass` reduces boilerplate and improves readability.\n",
    "\n",
    "Use it when your class is mainly “data with light behavior”.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "90472002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JobConfigDC(job_name='daily_sales_load', retries=5, timeout_seconds=60)\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class JobConfigDC:\n",
    "    job_name: str\n",
    "    retries: int = 3\n",
    "    timeout_seconds: int = 60\n",
    "\n",
    "\n",
    "cfg = JobConfigDC(job_name=\"daily_sales_load\", retries=5)\n",
    "print(cfg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358d94d3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "- **Classes** model reusable components; **instances** hold specific configurations.\n",
    "- `__init__` sets instance state; `self` points to the current object.\n",
    "- **Instance attributes** differ per object; **class attributes** are shared defaults.\n",
    "- `@classmethod` is great for **alternative constructors** (`from_config`, `from_env`).\n",
    "- `@staticmethod` is a **namespaced helper**.\n",
    "- **Inheritance + `super()`** reuse and extend behavior.\n",
    "- **Polymorphism** enables pluggable pipelines: “same interface, different implementation”.\n",
    "- **Encapsulation** with `@property` helps enforce invariants.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9540bef3",
   "metadata": {},
   "source": [
    "> Content created by **Carlos Cruz-Maldonado**.  \n",
    "> Updated with additional best practices and Data Engineering examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97f10655",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0181bd8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting run_job.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile run_job.py\n",
    "import argparse\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class JobArgs:\n",
    "    source_id: str\n",
    "    full_load: bool\n",
    "    job_id: str\n",
    "    output_dir: Path\n",
    "    retries: int\n",
    "    \n",
    "    \n",
    "def run_pipeline(steps, data):\n",
    "    for step in steps:\n",
    "        data = step.run(data)\n",
    "    return data\n",
    "\n",
    "\n",
    "class MultiplyAmount(NamedStep):\n",
    "    def __init__(self, name, factor):\n",
    "        super().__init__(name=name)\n",
    "        self.factor = factor\n",
    "\n",
    "    def run(self, rows):\n",
    "        self._log(f\"Multiplying amount by {self.factor}\")\n",
    "        out = []\n",
    "        for r in rows:\n",
    "            r2 = dict(r)\n",
    "            r2[\"amount\"] = r2[\"amount\"] * self.factor\n",
    "            out.append(r2)\n",
    "        return out\n",
    "\n",
    "\n",
    "def main() -> None:\n",
    "    \n",
    "    ### Setting arguments\n",
    "    \n",
    "    parser = argparse.ArgumentParser(\n",
    "        prog=\"run_job.py\",\n",
    "        description=\"Run extraction pipeline.\"\n",
    "        )\n",
    "    \n",
    "    parser.add_argument(\"--source-id\", choices=[\"csv\", \"parquet\", \"sftp\"], required=True, help=\"Metadata key for source extraction.\") # Required argument\n",
    "    parser.add_argument(\"--full-load\", action=\"store_true\", help=\"Specifies if full load will be executed.\") # Boolean argument\n",
    "    parser.add_argument(\"--job-id\", required=True, help=\"Job ID for metadata retrieval.\")\n",
    "    parser.add_argument(\"--output-dir\", default=\".\", help=\"Where output is being written.\")\n",
    "    \n",
    "    parser.add_argument(\"--retries\", type=int, default=3)#\n",
    "    \n",
    "\n",
    "    args = parser.parse_args()\n",
    "    print(\"Parsed arguments:\")\n",
    "    print(f\"  job_id = {args.job_id}\")\n",
    "    print(f\"  source_id = {args.source_id}\")\n",
    "    print(f\"  retries = {args.retries, type(args.retries)}\")\n",
    "    \n",
    "    job_args = JobArgs(\n",
    "        source_id=args.source_id,\n",
    "        full_load=args.full_load,\n",
    "        job_id=args.job_id,\n",
    "        output_dir=Path(args.output_dir),\n",
    "        retries=args.retries\n",
    "    )\n",
    "    \n",
    "    print(job_args)\n",
    "    \n",
    "    ### Begin Job\n",
    "    \n",
    "    # example run\n",
    "\n",
    "    pipeline = [\n",
    "        FilterColumns(name=\"cols\", keep_columns=[\"id\", \"amount\"]),\n",
    "        MultiplyAmount(name=\"mul\", factor=2),\n",
    "        AddConstantColumn(column_name=\"source\", value=\"batch_2026_01_27\"),\n",
    "    ]\n",
    "\n",
    "    rows = [{\"id\": 1, \"amount\": 10, \"country\": \"MX\"}, {\"id\": 2, \"amount\": 15, \"country\": \"US\"}]\n",
    "    print(run_pipeline(pipeline, rows))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4baae361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: run_job.py [-h] --source-id SOURCE_ID [--full-load] --job-id JOB_ID\n",
      "                  [--output-dir OUTPUT_DIR]\n",
      "\n",
      "Run extraction pipeline.\n",
      "\n",
      "options:\n",
      "  -h, --help            show this help message and exit\n",
      "  --source-id SOURCE_ID\n",
      "                        Metadata key for source extraction.\n",
      "  --full-load           Specifies if full load will be executed.\n",
      "  --job-id JOB_ID       Job ID for metadata retrieval.\n",
      "  --output-dir OUTPUT_DIR\n",
      "                        Where output is being written.\n"
     ]
    }
   ],
   "source": [
    "! python run_job.py -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c331d875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed arguments:\n",
      "  job_id = PS01\n",
      "  source_id = csv\n",
      "  retries = (4, <class 'int'>)\n",
      "JobArgs(source_id='csv', full_load=True, job_id='PS01', output_dir=PosixPath('.'), retries=4)\n"
     ]
    }
   ],
   "source": [
    "! python run_job.py --job-id PS01 --source-id csv --full-load --retries 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053bccde",
   "metadata": {},
   "outputs": [],
   "source": [
    "step.run(job_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3898e0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "from pathlib import Path\n",
    "\n",
    "DB_PATH = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803d8dfe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
