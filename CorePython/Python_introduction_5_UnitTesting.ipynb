{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4376ee1f",
   "metadata": {},
   "source": [
    "# Unit Testing Basics\n",
    "\n",
    "\n",
    "## Learning goals\n",
    "\n",
    "Unit tests are a way to “lock in” the behavior of your code. After this lesson, you should be able to:\n",
    "\n",
    "- Use `assert` as the core mechanism behind tests\n",
    "- Write tests with `pytest` (recommended in most projects)\n",
    "- Test both expected behavior and failure behavior (exceptions)\n",
    "- Reuse test data with fixtures\n",
    "- Test file output safely without touching your real filesystem\n",
    "- Understand why mocking / dependency injection matters for unit tests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab46c7b6",
   "metadata": {},
   "source": [
    "## What is a unit test and why do we care?\n",
    "\n",
    "A **unit test** is a small automated check for a small unit of code (often a single function). The goal is simple:\n",
    "given some input, verify the output or behavior is what we expect.\n",
    "\n",
    "In real projects, tests matter because code changes constantly: new features, bug fixes, refactors, performance work.\n",
    "Tests give you a safety net. If a change breaks something old, a test should fail quickly and tell you exactly what went wrong.\n",
    "\n",
    "- Unit tests should be **fast**, **repeatable**, and **deterministic**.\n",
    "- A unit test checks a **single behavior** (or a very small set of related behaviors).\n",
    "- Good tests act as **documentation**: they show what the code is supposed to do.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9781e4c",
   "metadata": {},
   "source": [
    "## Assertions (The core idea on what testing frameworks build on)\n",
    "\n",
    "At the heart of unit testing is the concept of an **assertion**: a statement that must be true.\n",
    "In Python, `assert condition` raises an `AssertionError` if the condition is false.\n",
    "\n",
    "Most testing frameworks (like `pytest`) are structured ways to run many `assert`s and report failures clearly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984b1109",
   "metadata": {},
   "source": [
    "### Example function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c63e0c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_email(email: str) -> str:\n",
    "    return email.strip().lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da96763f",
   "metadata": {},
   "source": [
    "### “Test” using plain assert:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f12d2e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_normalize_email_basic():\n",
    "    result = normalize_email(\"  Carlos@Example.COM \")\n",
    "    assert result == \"carlos@example.com\"\n",
    "    print(\"Test Passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43b05ac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Passed!\n"
     ]
    }
   ],
   "source": [
    "test_normalize_email_basic()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06911aac",
   "metadata": {},
   "source": [
    "## Arrange–Act–Assert\n",
    "\n",
    "When tests get longer, structure matters. A common pattern is **Arrange–Act–Assert (AAA)**:\n",
    "\n",
    "- **Arrange**: prepare inputs and context  \n",
    "- **Act**: call the function  \n",
    "- **Assert**: verify the result  \n",
    "\n",
    "This pattern makes tests easier to read later, especially when a test fails.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "59a95b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_tax(price: float, tax_rate: float) -> float:\n",
    "    return round(price * (1 + tax_rate), 2)\n",
    "\n",
    "def test_add_tax():\n",
    "    # Arrange\n",
    "    price = 100.0\n",
    "    tax_rate = 0.16\n",
    "\n",
    "    # Act\n",
    "    total = add_tax(price, tax_rate)\n",
    "\n",
    "    # Assert\n",
    "    assert total == 116.00\n",
    "    print(\"Test Passed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "05df993d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Passed!\n"
     ]
    }
   ],
   "source": [
    "test_add_tax()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01e2236",
   "metadata": {},
   "source": [
    "## pytest\n",
    "### Usually the best starting point\n",
    "\n",
    "In modern Python teams, `pytest` is commonly preferred because it keeps tests readable:\n",
    "\n",
    "- You write tests as normal functions\n",
    "- You use plain `assert`\n",
    "- It gives good failure messages (diffs, values, traces)\n",
    "- It supports fixtures for reusable setup\n",
    "\n",
    "### Typical structure:\n",
    "\n",
    "```\n",
    "project/\n",
    "  src/\n",
    "    pricing.py\n",
    "  tests/\n",
    "    test_pricing.py\n",
    "```\n",
    "\n",
    "### How to run:\n",
    "\n",
    "- `pytest -q`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ef02ad",
   "metadata": {},
   "source": [
    "### Example code: business rule + validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dbcc6920",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/workspaces/PythonAcademy/CorePython'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f83b645e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspaces/PythonAcademy\n"
     ]
    }
   ],
   "source": [
    "%cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9a124aa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/workspaces/PythonAcademy'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "66a9b3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.princing as pricing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f62b1b28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80.0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pricing.apply_discount(100,0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "74309597",
   "metadata": {},
   "outputs": [],
   "source": [
    "%mkdir src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fff278a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/pricing.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/pricing.py\n",
    "def apply_discount(price: float, discount: float) -> float:\n",
    "    \"\"\"\n",
    "    discount in range [0, 1]\n",
    "    \"\"\"\n",
    "    if not 0 <= discount <= 1:\n",
    "        raise ValueError(\"discount must be between 0 and 1\")\n",
    "    return round(price * (1 - discount), 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843b78db",
   "metadata": {},
   "source": [
    "### Tests (pytest style)\n",
    "\n",
    "> Note: In a real project, this code would live in `tests/test_pricing.py` and you would run it with `pytest`.\n",
    "> Here I show the standard pytest syntax.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8e723440",
   "metadata": {},
   "outputs": [],
   "source": [
    "%mkdir tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d255b9b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting tests/test_pricing.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile tests/test_pricing.py\n",
    "# pytest example (place each of this examples inside tests/test_pricing.py)\n",
    "import pytest\n",
    "\n",
    "#from src.pricing import apply_discount\n",
    "\n",
    "def apply_discount(price: float, discount: float) -> float:\n",
    "    \"\"\"\n",
    "    discount in range [0, 1]\n",
    "    \"\"\"\n",
    "    if not 0 <= discount <= 1:\n",
    "        raise ValueError(\"discount must be between 0 and 1\")\n",
    "    return round(price * (1 - discount), 2)\n",
    "\n",
    "def test_apply_discount_happy_path():\n",
    "    assert apply_discount(100, 0.2) == 80.0\n",
    "\n",
    "def test_apply_discount_zero_discount():\n",
    "    assert apply_discount(100, 0.0) == 100.0\n",
    "\n",
    "def test_apply_discount_invalid_discount_raises():\n",
    "    with pytest.raises(ValueError):\n",
    "        apply_discount(100, 1.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77f5d69e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspaces/PythonAcademy\n"
     ]
    }
   ],
   "source": [
    "%cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3adeab9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.11.14, pytest-9.0.2, pluggy-1.6.0\n",
      "rootdir: /workspaces/PythonAcademy\n",
      "plugins: Faker-40.1.2\n",
      "collected 3 items                                                              \u001b[0m\n",
      "\n",
      "tests/test_pricing.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m3 passed\u001b[0m\u001b[32m in 0.05s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ab11c9",
   "metadata": {},
   "source": [
    "## Happy path vs edge cases\n",
    "\n",
    "A common beginner mistake is writing only one test for a function and assuming it’s enough.\n",
    "Real bugs often live in **edge cases**.\n",
    "\n",
    "From a data engineering perspective, edge cases often include:\n",
    "- empty strings\n",
    "- missing values\n",
    "- malformed numeric formats\n",
    "- unexpected whitespace\n",
    "- negative numbers or invalid ranges\n",
    "\n",
    "Below is a small example where we decide (explicitly) what happens when `b == 0`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a33b4f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_divide(a: float, b: float) -> float:\n",
    "    if b == 0:\n",
    "        raise ZeroDivisionError(\"b must not be zero\")\n",
    "    return a / b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "15f31640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing tests/test_safe_divide.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile tests/test_safe_divide.py\n",
    "# pytest example tests\n",
    "import pytest\n",
    "\n",
    "def safe_divide(a: float, b: float) -> float:\n",
    "    if b == 0:\n",
    "        raise ZeroDivisionError(\"b must not be zero\")\n",
    "    return a / b\n",
    "\n",
    "def test_safe_divide_normal():\n",
    "    assert safe_divide(10, 2) == 5\n",
    "\n",
    "def test_safe_divide_raises_on_zero():\n",
    "    with pytest.raises(ZeroDivisionError):\n",
    "        safe_divide(10, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c9ee5dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspaces/PythonAcademy\n"
     ]
    }
   ],
   "source": [
    "cd ../\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b8c4e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.11.14, pytest-9.0.2, pluggy-1.6.0\n",
      "rootdir: /workspaces/PythonAcademy\n",
      "plugins: Faker-40.1.2\n",
      "collected 2 items / 1 deselected / 1 selected                                  \u001b[0m\n",
      "\n",
      "tests/test_safe_divide.py \u001b[32m.\u001b[0m\u001b[32m                                              [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m======================= \u001b[32m\u001b[1m1 passed\u001b[0m, \u001b[33m1 deselected\u001b[0m\u001b[32m in 0.06s\u001b[0m\u001b[32m ========================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest tests/test_safe_divide.py -k \"test_safe_divide_raises\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4d7beb5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.11.14, pytest-9.0.2, pluggy-1.6.0\n",
      "rootdir: /workspaces/PythonAcademy\n",
      "plugins: Faker-40.1.2\n",
      "collected 2 items                                                              \u001b[0m\n",
      "\n",
      "tests/test_safe_divide.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                             [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 0.04s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest tests/test_safe_divide.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "869183a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.11.14, pytest-9.0.2, pluggy-1.6.0 -- /workspaces/PythonAcademy/.venv/bin/python\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /workspaces/PythonAcademy\n",
      "plugins: Faker-40.1.2\n",
      "collected 5 items                                                              \u001b[0m\n",
      "\n",
      "tests/test_pricing.py::test_apply_discount_happy_path \u001b[32mPASSED\u001b[0m\u001b[32m             [ 20%]\u001b[0m\n",
      "tests/test_pricing.py::test_apply_discount_zero_discount \u001b[32mPASSED\u001b[0m\u001b[32m          [ 40%]\u001b[0m\n",
      "tests/test_pricing.py::test_apply_discount_invalid_discount_raises \u001b[32mPASSED\u001b[0m\u001b[32m [ 60%]\u001b[0m\n",
      "tests/test_safe_divide.py::test_safe_divide_normal \u001b[32mPASSED\u001b[0m\u001b[32m                [ 80%]\u001b[0m\n",
      "tests/test_safe_divide.py::test_safe_divide_raises_on_zero \u001b[32mPASSED\u001b[0m\u001b[32m        [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m5 passed\u001b[0m\u001b[32m in 0.05s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pytest -vv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9fd0782c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: pytest [options] [file_or_dir] [file_or_dir] [...]\n",
      "\n",
      "positional arguments:\n",
      "  file_or_dir\n",
      "\n",
      "general:\n",
      "  -k EXPRESSION         Only run tests which match the given substring\n",
      "                        expression. An expression is a Python evaluable\n",
      "                        expression where all names are substring-matched against\n",
      "                        test names and their parent classes. Example: -k\n",
      "                        'test_method or test_other' matches all test functions\n",
      "                        and classes whose name contains 'test_method' or\n",
      "                        'test_other', while -k 'not test_method' matches those\n",
      "                        that don't contain 'test_method' in their names. -k 'not\n",
      "                        test_method and not test_other' will eliminate the\n",
      "                        matches. Additionally keywords are matched to classes\n",
      "                        and functions containing extra names in their\n",
      "                        'extra_keyword_matches' set, as well as functions which\n",
      "                        have names assigned directly to them. The matching is\n",
      "                        case-insensitive.\n",
      "  -m MARKEXPR           Only run tests matching given mark expression. For\n",
      "                        example: -m 'mark1 and not mark2'.\n",
      "  --markers             show markers (builtin, plugin and per-project ones).\n",
      "  -x, --exitfirst       Exit instantly on first error or failed test\n",
      "  --maxfail=num         Exit after first num failures or errors\n",
      "  --strict-config       Enables the strict_config option\n",
      "  --strict-markers      Enables the strict_markers option\n",
      "  --strict              Enables the strict option\n",
      "  --fixtures, --funcargs\n",
      "                        Show available fixtures, sorted by plugin appearance\n",
      "                        (fixtures with leading '_' are only shown with '-v')\n",
      "  --fixtures-per-test   Show fixtures per test\n",
      "  --pdb                 Start the interactive Python debugger on errors or\n",
      "                        KeyboardInterrupt\n",
      "  --pdbcls=modulename:classname\n",
      "                        Specify a custom interactive Python debugger for use\n",
      "                        with --pdb.For example:\n",
      "                        --pdbcls=IPython.terminal.debugger:TerminalPdb\n",
      "  --trace               Immediately break when running each test\n",
      "  --capture=method      Per-test capturing method: one of fd|sys|no|tee-sys\n",
      "  -s                    Shortcut for --capture=no\n",
      "  --runxfail            Report the results of xfail tests as if they were not\n",
      "                        marked\n",
      "  --lf, --last-failed   Rerun only the tests that failed at the last run (or all\n",
      "                        if none failed)\n",
      "  --ff, --failed-first  Run all tests, but run the last failures first. This may\n",
      "                        re-order tests and thus lead to repeated fixture\n",
      "                        setup/teardown.\n",
      "  --nf, --new-first     Run tests from new files first, then the rest of the\n",
      "                        tests sorted by file mtime\n",
      "  --cache-show=[CACHESHOW]\n",
      "                        Show cache contents, don't perform collection or tests.\n",
      "                        Optional argument: glob (default: '*').\n",
      "  --cache-clear         Remove all cache contents at start of test run\n",
      "  --lfnf={all,none}, --last-failed-no-failures={all,none}\n",
      "                        With ``--lf``, determines whether to execute tests when\n",
      "                        there are no previously (known) failures or when no\n",
      "                        cached ``lastfailed`` data was found. ``all`` (the\n",
      "                        default) runs the full test suite again. ``none`` just\n",
      "                        emits a message about no known failures and exits\n",
      "                        successfully.\n",
      "  --sw, --stepwise      Exit on test failure and continue from last failing test\n",
      "                        next time\n",
      "  --sw-skip, --stepwise-skip\n",
      "                        Ignore the first failing test but stop on the next\n",
      "                        failing test. Implicitly enables --stepwise.\n",
      "  --sw-reset, --stepwise-reset\n",
      "                        Resets stepwise state, restarting the stepwise workflow.\n",
      "                        Implicitly enables --stepwise.\n",
      "\n",
      "Reporting:\n",
      "  --durations=N         Show N slowest setup/test durations (N=0 for all)\n",
      "  --durations-min=N     Minimal duration in seconds for inclusion in slowest\n",
      "                        list. Default: 0.005 (or 0.0 if -vv is given).\n",
      "  -v, --verbose         Increase verbosity\n",
      "  --no-header           Disable header\n",
      "  --no-summary          Disable summary\n",
      "  --no-fold-skipped     Do not fold skipped tests in short summary.\n",
      "  --force-short-summary\n",
      "                        Force condensed summary output regardless of verbosity\n",
      "                        level.\n",
      "  -q, --quiet           Decrease verbosity\n",
      "  --verbosity=VERBOSE   Set verbosity. Default: 0.\n",
      "  -r chars              Show extra test summary info as specified by chars:\n",
      "                        (f)ailed, (E)rror, (s)kipped, (x)failed, (X)passed,\n",
      "                        (p)assed, (P)assed with output, (a)ll except passed\n",
      "                        (p/P), or (A)ll. (w)arnings are enabled by default (see\n",
      "                        --disable-warnings), 'N' can be used to reset the list.\n",
      "                        (default: 'fE').\n",
      "  --disable-warnings, --disable-pytest-warnings\n",
      "                        Disable warnings summary\n",
      "  -l, --showlocals      Show locals in tracebacks (disabled by default)\n",
      "  --no-showlocals       Hide locals in tracebacks (negate --showlocals passed\n",
      "                        through addopts)\n",
      "  --tb=style            Traceback print mode (auto/long/short/line/native/no)\n",
      "  --xfail-tb            Show tracebacks for xfail (as long as --tb != no)\n",
      "  --show-capture={no,stdout,stderr,log,all}\n",
      "                        Controls how captured stdout/stderr/log is shown on\n",
      "                        failed tests. Default: all.\n",
      "  --full-trace          Don't cut any tracebacks (default is to cut)\n",
      "  --color=color         Color terminal output (yes/no/auto)\n",
      "  --code-highlight={yes,no}\n",
      "                        Whether code should be highlighted (only if --color is\n",
      "                        also enabled). Default: yes.\n",
      "  --pastebin=mode       Send failed|all info to bpaste.net pastebin service\n",
      "  --junit-xml=path      Create junit-xml style report file at given path\n",
      "  --junit-prefix=str    Prepend prefix to classnames in junit-xml output\n",
      "\n",
      "pytest-warnings:\n",
      "  -W PYTHONWARNINGS, --pythonwarnings=PYTHONWARNINGS\n",
      "                        Set which warnings to report, see -W option of Python\n",
      "                        itself\n",
      "\n",
      "collection:\n",
      "  --collect-only, --co  Only collect tests, don't execute them\n",
      "  --pyargs              Try to interpret all arguments as Python packages\n",
      "  --ignore=path         Ignore path during collection (multi-allowed)\n",
      "  --ignore-glob=path    Ignore path pattern during collection (multi-allowed)\n",
      "  --deselect=nodeid_prefix\n",
      "                        Deselect item (via node id prefix) during collection\n",
      "                        (multi-allowed)\n",
      "  --confcutdir=dir      Only load conftest.py's relative to specified dir\n",
      "  --noconftest          Don't load any conftest.py files\n",
      "  --keep-duplicates     Keep duplicate tests\n",
      "  --collect-in-virtualenv\n",
      "                        Don't ignore tests in a local virtualenv directory\n",
      "  --continue-on-collection-errors\n",
      "                        Force test execution even if collection errors occur\n",
      "  --import-mode={prepend,append,importlib}\n",
      "                        Prepend/append to sys.path when importing test modules\n",
      "                        and conftest files. Default: prepend.\n",
      "  --doctest-modules     Run doctests in all .py modules\n",
      "  --doctest-report={none,cdiff,ndiff,udiff,only_first_failure}\n",
      "                        Choose another output format for diffs on doctest\n",
      "                        failure\n",
      "  --doctest-glob=pat    Doctests file matching pattern, default: test*.txt\n",
      "  --doctest-ignore-import-errors\n",
      "                        Ignore doctest collection errors\n",
      "  --doctest-continue-on-failure\n",
      "                        For a given doctest, continue to run after the first\n",
      "                        failure\n",
      "\n",
      "test session debugging and configuration:\n",
      "  -c FILE, --config-file=FILE\n",
      "                        Load configuration from `FILE` instead of trying to\n",
      "                        locate one of the implicit configuration files.\n",
      "  --rootdir=ROOTDIR     Define root directory for tests. Can be relative path:\n",
      "                        'root_dir', './root_dir', 'root_dir/another_dir/';\n",
      "                        absolute path: '/home/user/root_dir'; path with\n",
      "                        variables: '$HOME/root_dir'.\n",
      "  --basetemp=dir        Base temporary directory for this test run. (Warning:\n",
      "                        this directory is removed if it exists.)\n",
      "  -V, --version         Display pytest version and information about plugins.\n",
      "                        When given twice, also display information about\n",
      "                        plugins.\n",
      "  -h, --help            Show help message and configuration info\n",
      "  -p name               Early-load given plugin module name or entry point\n",
      "                        (multi-allowed). To avoid loading of plugins, use the\n",
      "                        `no:` prefix, e.g. `no:doctest`. See also --disable-\n",
      "                        plugin-autoload.\n",
      "  --disable-plugin-autoload\n",
      "                        Disable plugin auto-loading through entry point\n",
      "                        packaging metadata. Only plugins explicitly specified in\n",
      "                        -p or env var PYTEST_PLUGINS will be loaded.\n",
      "  --trace-config        Trace considerations of conftest.py files\n",
      "  --debug=[DEBUG_FILE_NAME]\n",
      "                        Store internal tracing debug information in this log\n",
      "                        file. This file is opened with 'w' and truncated as a\n",
      "                        result, care advised. Default: pytestdebug.log.\n",
      "  -o OVERRIDE_INI, --override-ini=OVERRIDE_INI\n",
      "                        Override configuration option with \"option=value\" style,\n",
      "                        e.g. `-o strict_xfail=True -o cache_dir=cache`.\n",
      "  --assert=MODE         Control assertion debugging tools.\n",
      "                        'plain' performs no assertion debugging.\n",
      "                        'rewrite' (the default) rewrites assert statements in\n",
      "                        test modules on import to provide assert expression\n",
      "                        information.\n",
      "  --setup-only          Only setup fixtures, do not execute tests\n",
      "  --setup-show          Show setup of fixtures while executing tests\n",
      "  --setup-plan          Show what fixtures and tests would be executed but don't\n",
      "                        execute anything\n",
      "\n",
      "logging:\n",
      "  --log-level=LEVEL     Level of messages to catch/display. Not set by default,\n",
      "                        so it depends on the root/parent log handler's effective\n",
      "                        level, where it is \"WARNING\" by default.\n",
      "  --log-format=LOG_FORMAT\n",
      "                        Log format used by the logging module\n",
      "  --log-date-format=LOG_DATE_FORMAT\n",
      "                        Log date format used by the logging module\n",
      "  --log-cli-level=LOG_CLI_LEVEL\n",
      "                        CLI logging level\n",
      "  --log-cli-format=LOG_CLI_FORMAT\n",
      "                        Log format used by the logging module\n",
      "  --log-cli-date-format=LOG_CLI_DATE_FORMAT\n",
      "                        Log date format used by the logging module\n",
      "  --log-file=LOG_FILE   Path to a file when logging will be written to\n",
      "  --log-file-mode={w,a}\n",
      "                        Log file open mode\n",
      "  --log-file-level=LOG_FILE_LEVEL\n",
      "                        Log file logging level\n",
      "  --log-file-format=LOG_FILE_FORMAT\n",
      "                        Log format used by the logging module\n",
      "  --log-file-date-format=LOG_FILE_DATE_FORMAT\n",
      "                        Log date format used by the logging module\n",
      "  --log-auto-indent=LOG_AUTO_INDENT\n",
      "                        Auto-indent multiline messages passed to the logging\n",
      "                        module. Accepts true|on, false|off or an integer.\n",
      "  --log-disable=LOGGER_DISABLE\n",
      "                        Disable a logger by name. Can be passed multiple times.\n",
      "\n",
      "[pytest] configuration options in the first pytest.toml|pytest.ini|tox.ini|setup.cfg|pyproject.toml file found:\n",
      "\n",
      "  markers (linelist):   Register new markers for test functions\n",
      "  empty_parameter_set_mark (string):\n",
      "                        Default marker for empty parametersets\n",
      "  strict_config (bool): Any warnings encountered while parsing the `pytest`\n",
      "                        section of the configuration file raise errors\n",
      "  strict_markers (bool):\n",
      "                        Markers not registered in the `markers` section of the\n",
      "                        configuration file raise errors\n",
      "  strict (bool):        Enables all strictness options, currently:\n",
      "                        strict_config, strict_markers, strict_xfail,\n",
      "                        strict_parametrization_ids\n",
      "  filterwarnings (linelist):\n",
      "                        Each line specifies a pattern for\n",
      "                        warnings.filterwarnings. Processed after\n",
      "                        -W/--pythonwarnings.\n",
      "  norecursedirs (args): Directory patterns to avoid for recursion\n",
      "  testpaths (args):     Directories to search for tests when no files or\n",
      "                        directories are given on the command line\n",
      "  collect_imported_tests (bool):\n",
      "                        Whether to collect tests in imported modules outside\n",
      "                        `testpaths`\n",
      "  consider_namespace_packages (bool):\n",
      "                        Consider namespace packages when resolving module names\n",
      "                        during import\n",
      "  usefixtures (args):   List of default fixtures to be used with this project\n",
      "  python_files (args):  Glob-style file patterns for Python test module\n",
      "                        discovery\n",
      "  python_classes (args):\n",
      "                        Prefixes or glob names for Python test class discovery\n",
      "  python_functions (args):\n",
      "                        Prefixes or glob names for Python test function and\n",
      "                        method discovery\n",
      "  disable_test_id_escaping_and_forfeit_all_rights_to_community_support (bool):\n",
      "                        Disable string escape non-ASCII characters, might cause\n",
      "                        unwanted side effects(use at your own risk)\n",
      "  strict_parametrization_ids (bool):\n",
      "                        Emit an error if non-unique parameter set IDs are\n",
      "                        detected\n",
      "  console_output_style (string):\n",
      "                        Console output: \"classic\", or with additional progress\n",
      "                        information (\"progress\" (percentage) | \"count\" |\n",
      "                        \"progress-even-when-capture-no\" (forces progress even\n",
      "                        when capture=no)\n",
      "  verbosity_test_cases (string):\n",
      "                        Specify a verbosity level for test case execution,\n",
      "                        overriding the main level. Higher levels will provide\n",
      "                        more detailed information about each test case executed.\n",
      "  strict_xfail (bool):  Default for the strict parameter of xfail markers when\n",
      "                        not given explicitly (default: False) (alias:\n",
      "                        xfail_strict)\n",
      "  tmp_path_retention_count (string):\n",
      "                        How many sessions should we keep the `tmp_path`\n",
      "                        directories, according to `tmp_path_retention_policy`.\n",
      "  tmp_path_retention_policy (string):\n",
      "                        Controls which directories created by the `tmp_path`\n",
      "                        fixture are kept around, based on test outcome.\n",
      "                        (all/failed/none)\n",
      "  enable_assertion_pass_hook (bool):\n",
      "                        Enables the pytest_assertion_pass hook. Make sure to\n",
      "                        delete any previously generated pyc cache files.\n",
      "  truncation_limit_lines (string):\n",
      "                        Set threshold of LINES after which truncation will take\n",
      "                        effect\n",
      "  truncation_limit_chars (string):\n",
      "                        Set threshold of CHARS after which truncation will take\n",
      "                        effect\n",
      "  verbosity_assertions (string):\n",
      "                        Specify a verbosity level for assertions, overriding the\n",
      "                        main level. Higher levels will provide more detailed\n",
      "                        explanation when an assertion fails.\n",
      "  junit_suite_name (string):\n",
      "                        Test suite name for JUnit report\n",
      "  junit_logging (string):\n",
      "                        Write captured log messages to JUnit report: one of\n",
      "                        no|log|system-out|system-err|out-err|all\n",
      "  junit_log_passing_tests (bool):\n",
      "                        Capture log information for passing tests to JUnit\n",
      "                        report:\n",
      "  junit_duration_report (string):\n",
      "                        Duration time to report: one of total|call\n",
      "  junit_family (string):\n",
      "                        Emit XML for schema: one of legacy|xunit1|xunit2\n",
      "  doctest_optionflags (args):\n",
      "                        Option flags for doctests\n",
      "  doctest_encoding (string):\n",
      "                        Encoding used for doctest files\n",
      "  cache_dir (string):   Cache directory path\n",
      "  log_level (string):   Default value for --log-level\n",
      "  log_format (string):  Default value for --log-format\n",
      "  log_date_format (string):\n",
      "                        Default value for --log-date-format\n",
      "  log_cli (bool):       Enable log display during test run (also known as \"live\n",
      "                        logging\")\n",
      "  log_cli_level (string):\n",
      "                        Default value for --log-cli-level\n",
      "  log_cli_format (string):\n",
      "                        Default value for --log-cli-format\n",
      "  log_cli_date_format (string):\n",
      "                        Default value for --log-cli-date-format\n",
      "  log_file (string):    Default value for --log-file\n",
      "  log_file_mode (string):\n",
      "                        Default value for --log-file-mode\n",
      "  log_file_level (string):\n",
      "                        Default value for --log-file-level\n",
      "  log_file_format (string):\n",
      "                        Default value for --log-file-format\n",
      "  log_file_date_format (string):\n",
      "                        Default value for --log-file-date-format\n",
      "  log_auto_indent (string):\n",
      "                        Default value for --log-auto-indent\n",
      "  faulthandler_timeout (string):\n",
      "                        Dump the traceback of all threads if a test takes more\n",
      "                        than TIMEOUT seconds to finish\n",
      "  faulthandler_exit_on_timeout (bool):\n",
      "                        Exit the test process if a test takes more than\n",
      "                        faulthandler_timeout seconds to finish\n",
      "  verbosity_subtests (string):\n",
      "                        Specify verbosity level for subtests. Higher levels will\n",
      "                        generate output for passed subtests. Failed subtests are\n",
      "                        always reported.\n",
      "  addopts (args):       Extra command line options\n",
      "  minversion (string):  Minimally required pytest version\n",
      "  pythonpath (paths):   Add paths to sys.path\n",
      "  required_plugins (args):\n",
      "                        Plugins that must be present for pytest to run\n",
      "\n",
      "Environment variables:\n",
      "  CI                       When set to a non-empty value, pytest knows it is running in a CI process and does not truncate summary info\n",
      "  BUILD_NUMBER             Equivalent to CI\n",
      "  PYTEST_ADDOPTS           Extra command line options\n",
      "  PYTEST_PLUGINS           Comma-separated plugins to load during startup\n",
      "  PYTEST_DISABLE_PLUGIN_AUTOLOAD Set to disable plugin auto-loading\n",
      "  PYTEST_DEBUG             Set to enable debug tracing of pytest's internals\n",
      "  PYTEST_DEBUG_TEMPROOT    Override the system temporary directory\n",
      "  PYTEST_THEME             The Pygments style to use for code output\n",
      "  PYTEST_THEME_MODE        Set the PYTEST_THEME to be either 'dark' or 'light'\n",
      "\n",
      "\n",
      "to see available markers type: pytest --markers\n",
      "to see available fixtures type: pytest --fixtures\n",
      "(shown according to specified file_or_dir or current dir if not specified; fixtures with leading '_' are only shown with the '-v' option\n"
     ]
    }
   ],
   "source": [
    "!pytest --help"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88865bf",
   "metadata": {},
   "source": [
    "## Fixtures\n",
    "\n",
    "A **fixture** is reusable test setup. Instead of repeating the same data in every test,\n",
    "you define it once and inject it into any test that needs it.\n",
    "\n",
    "Tip: This is especially helpful in ETL contexts where “sample records” show up everywhere.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a5d4dc3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing tests/test_total_amount.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile tests/test_total_amount.py\n",
    "# pytest fixture example\n",
    "import pytest\n",
    "\n",
    "@pytest.fixture\n",
    "def sample_orders():\n",
    "    return [\n",
    "        {\"id\": 1, \"amount\": 100.0},\n",
    "        {\"id\": 2, \"amount\": 250.5},\n",
    "        {\"id\": 3, \"amount\": 0.0},\n",
    "    ]\n",
    "\n",
    "def total_amount(orders):\n",
    "    return sum(o[\"amount\"] for o in orders)\n",
    "\n",
    "def test_total_amount(sample_orders):\n",
    "    assert total_amount(sample_orders) == 350.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "132477e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.11.14, pytest-9.0.2, pluggy-1.6.0 -- /workspaces/PythonAcademy/.venv/bin/python\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /workspaces/PythonAcademy\n",
      "plugins: Faker-40.1.2\n",
      "collected 1 item                                                               \u001b[0m\n",
      "\n",
      "tests/test_total_amount.py::test_total_amount \u001b[32mPASSED\u001b[0m\u001b[32m                     [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.04s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest -vv tests/test_total_amount.py "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6efca2",
   "metadata": {},
   "source": [
    "## Testing file output safely\n",
    "\n",
    "ETL code often reads/writes files. A bad testing practice is writing to real paths like `./output/report.txt`\n",
    "because tests can:\n",
    "\n",
    "- overwrite real files\n",
    "- conflict with other runs\n",
    "- behave differently on different machines\n",
    "\n",
    "Pytest provides `tmp_path`, a temporary directory unique to the test run.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef082f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def write_report(path: Path, content: str) -> None:\n",
    "    path.write_text(content, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f08def",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytest example using tmp_path\n",
    "def test_write_report(tmp_path):\n",
    "    report_path = tmp_path / \"report.txt\"\n",
    "    write_report(report_path, \"hello\")\n",
    "\n",
    "    assert report_path.exists()\n",
    "    assert report_path.read_text(encoding=\"utf-8\") == \"hello\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bef753",
   "metadata": {},
   "source": [
    "## Mocking and dependency injection (unit tests should not call the real world)\n",
    "\n",
    "Unit tests should be fast and stable. External systems (network calls, DB queries, cloud storage)\n",
    "are slow and can fail for reasons unrelated to your code.\n",
    "\n",
    "A clean approach is **dependency injection**: pass the dependency in. Then in tests, you pass a fake version.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61175b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_user(user: dict) -> dict:\n",
    "    return {\"id\": user[\"id\"], \"email\": user[\"email\"].strip().lower()}\n",
    "\n",
    "def process_users(fetch_users_func):\n",
    "    users = fetch_users_func()\n",
    "    return [transform_user(u) for u in users]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c044349",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_process_users_with_fake_fetch():\n",
    "    def fake_fetch():\n",
    "        return [{\"id\": 1, \"email\": \"  A@B.COM \"}]\n",
    "\n",
    "    result = process_users(fake_fetch)\n",
    "    assert result == [{\"id\": 1, \"email\": \"a@b.com\"}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff0dbab",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "These exercises are designed to practice:\n",
    "- normal expected behavior\n",
    "- exception testing\n",
    "- file round-trip testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7304973",
   "metadata": {},
   "source": [
    "## Exercise 1: Test input cleaning + exception behavior\n",
    "\n",
    "### Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64311a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_country(country: str) -> str:\n",
    "    country = country.strip().title()\n",
    "    if not country:\n",
    "        raise ValueError(\"country must not be empty\")\n",
    "    return country\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c89637",
   "metadata": {},
   "source": [
    "### Task\n",
    "\n",
    "Write 3 tests:\n",
    "\n",
    "- `\"  mexico \"` → `\"Mexico\"`\n",
    "- `\"Canada\"` → `\"Canada\"`\n",
    "- `\"\"` raises `ValueError`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5396681c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "# Have a little help, fill in the required code\n",
    "def test_clean_country_normal():\n",
    "    pass\n",
    "\n",
    "def test_clean_country_already_clean():\n",
    "    pass\n",
    "\n",
    "def test_clean_country_empty_raises():\n",
    "    with pytest.raises(ValueError):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9d2f2f",
   "metadata": {},
   "source": [
    "## Exercise 2: Test a mini ETL parsing function\n",
    "\n",
    "### Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4354bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_amount(value: str) -> float:\n",
    "    value = value.replace(\",\", \"\").strip()\n",
    "    return float(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c5d150",
   "metadata": {},
   "source": [
    "### Task\n",
    "\n",
    "- `\"1,234.50\"` → `1234.50`\n",
    "- `\" 0 \"` → `0.0`\n",
    "- `\"abc\"` raises `ValueError`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee242f31",
   "metadata": {},
   "source": [
    "## Exercise 3: File round-trip test with tmp_path\n",
    "\n",
    "### Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c068a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "def write_json(path: Path, data: dict) -> None: # Note it receives a Path object, not a path string\n",
    "    path.write_text(json.dumps(data), encoding=\"utf-8\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e7e6db",
   "metadata": {},
   "source": [
    "### Task\n",
    "\n",
    "Use `tmp_path` to verify:\n",
    "\n",
    "- file exists  \n",
    "- content loads back to the same dict  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ee72a9",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37cc81e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Fill in the spaces\n",
    "\n",
    "def test_write_json_roundtrip(tmp_path):\n",
    "    path = ___ / \"data.json\" # Needs to be a Path object! \n",
    "    payload = {\"a\": 1, \"b\": \"x\"}\n",
    "\n",
    "    write_json(path, payload)\n",
    "\n",
    "    assert ___.exists()\n",
    "    loaded = json.loads(___) # Ptss: ___.read_text(encoding=\"utf-8\")\n",
    "    assert ___ # Compare what you loaded with the default payload\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eadf5e3d",
   "metadata": {},
   "source": [
    "## Common mistakes (and how to talk about them)\n",
    "\n",
    "A lot of testing pain comes from a few repeating mistakes:\n",
    "\n",
    "- Writing tests that depend on the network / DB / real APIs\n",
    "- Using random test data without controlling it (seed it, or use fixed fixtures) # for seeds check ´random´ module or Numpy's random.seed()\n",
    "- Testing too many things in one test (hard to diagnose failures)\n",
    "- Tests that rely on execution order (tests should be independent)\n",
    "- Not testing failure behavior (exceptions, invalid input)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386cc15e",
   "metadata": {},
   "source": [
    "> Content created by [**Carlos Cruz-Maldonado**](https://www.linkedin.com/in/carloscruzmaldonado/).  \n",
    "> I am available to answer any questions or provide further assistance.   \n",
    "> Feel free to reach out to me at any time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
